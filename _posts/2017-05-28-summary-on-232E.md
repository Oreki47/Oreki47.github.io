---
author: Zhiyuan
comments: true
date: 2017-05-28
layout: post
title: Summary on 232E project
categories: General
tags:
- Lectures
---
So this 232E project 2 turns out to be a pain in the ass. Although I do think I learned a lot during the process.

I feel like some of the times I came up with bad implementations and later on just realized that it is not going to work. I guess I can only make progress through post-analysis.

-- First Glance

This project starts with parsing files. One thing that came to mind at the very beginning was to load  the '.txt' file with pandas and use regex for parsing. This turned out to be very inefficient, as the length of each row is different (each actor/actress starred in a different number of movies). If we were to use dataframe, a lot of Nan values (more than 50% of the cells are Nan) would have been created and this results in a very large file that is not suitable for later tasks.

At that time I suddenly realized I trapped myself in the idea that, since Pandas is so fast and convenient, it would be suitable to use it for every task that is related to data parsing and analyzing. This, of course, is a naive thought. I spent an afternoon trying to use pandas and it was in vain.

-- Thinking and Planning

I was a bit frustrated about what I achieved that afternoon and restart the whole thing with some simple planning. I listed out a couple things; what kind of files are needed; what's there structure; what kind of data structure should I use; how to achieve and so on. This turned out to be useful. And at this moment, I feel like I could have spent a bit more time on this. Also, I should be updating this planning note as I move forward. Of course, I ended up with something different than what I have planned. Yet having a note of any kind helps to keep tracks and plan further. I am a newbie on this matter as I have never finished a big enough project that requires much of planning. Mostly I just sat down and start coding, and it just worked. That's of course not saying I am smart or whatever, it's just what I have encountered in the past tend to have low complexity.

Well, it has been 15 days and I am back.

I spent a couple of days indulging in games and movies and now I am back to work. Essentially what I have learned the most from the project, as in retrospect, is that if I ever stop coding, my hands get rusty. This is obvious as I struggled a bit today plotting a 1-D Gaussian mixture model. I forgot a lot of commands and shortcuts and so on. But let's get back to the project and see what I have missed.

To continue, I followed the instruction and start creating edge list files. As it turned out, the file is humongous and requires some optimization before processing. Here I first noticed how memory in the computer works, and also realized the fact that my CPU is just an overkill, as it never goes above 20%. The first thing I noticed is how memory saving it can be by replacing strings with numerical values and use a dictionary as an index. This shrunk the file to 20% of its original size and facilitates graph constructing in R as well.

As such, I was able to create edge list in a reasonable time. Oh, actually I did another optimization that decreased the time required from a couple hours to a mere 30 minutes. As I was creating edge list. I used pandas concat/merge commands. These commands are not suitable for recursive use as it requires copying the object every turn. It is most suitable for occasional combining. At first, I merge my edge list every round, which becomes extremely slow as the size of the edge list increase. Given that I used two different strategies. The first one is merged in multiple layers. Here is what I did, I store the edge list into dozens of files and merge the files at the end. This turned out to be super effective and safe because if there is something wrong with the process, I can always come back from the break point. The second choice is simple and intuitive, by simply using generic merge of python list. This, on the other hand, does not make a copy of the original list and therefore is also super fast. I started using option 1 and later on switched to 2 because I just realized for a file that can be generated within 30 minutes, there is no need spitting them for an extra layer of safety.

Think I lost the motivation of writing this summary. I waited too long and should have done it when my memories are fresh.

Overall this was a very good experience as I start to care more about, not just the coding itself, but performance and efficiency. I aslo had a ton of experience on coding as well, which is less important, I guess. Hope I get an A in this class.

